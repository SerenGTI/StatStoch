%!TEX root = ../main.tex
\chapter{Wahrscheinlichkeitsräume}

\section{Allgemeine Definitionen}
\begin{itemize}
	\item Ein Wahrscheinlichkeitsraum besteht aus einem \emph{Grundraum} $\Omega$, einem System $\mathcal A$ von Teilmengen von $\Omega$ und der \emph{Wahrscheinlichkeitsverteilung} $P$.

	\item Der Grundraum $\Omega\neq \emptyset$ ist nichtleer und ist die Menge der Ergebnisse. Elemente $A\in\mathcal A$ heißen Ereignisse, ein Ereignis mit $|A|=1$ heißt Elementarereignis. Außerdem ist $\emptyset\in\mathcal A$ das \emph{unmögliche Ereignis} und $\Omega\in\mathcal A$ das sogenannte \emph{sichere Ereignis}.

	\item Die Wahrscheinlichkeitsverteilung ist eine Abbildung $P:\mathcal A\rightarrow \R$.

	\item Die Ereignismenge $\mathcal A$ muss unter den Mengenoperationen $\simpleset{\cap, \cup, \setminus}$ und Komplement abgeschlossen sein. Man nennt zwei Ereignisse $A, B\in\mathcal A$ \emph{unvereinbar}, wenn $A\cap B=\emptyset$ ($A$ und $B$ sind disjunkte Mengen). Für paarweise unvereinbare Ereignisse $A_i$ definieren wir $\bigcup_{i} A_i\coloneqq \sum_{i} A_i$.


	\item Eine \emph{Zufallsvariable} ist eine Abbildung $X:\Omega\rightarrow \R$, sie ordnet einem Ergebnis einen Wert zu. Zum Beispiel wäre die Augensumme beim Werfen mehrerer Würfel eine Zufallsvariable. Für eine Zufallsvariable $X$ sei $X^{-1}$ für eine beliebige Teilmenge $A\in\mathcal A$ definiert als das Urbild $X^{-1}(A)=\set{\omega\in\Omega}{X(\omega)\in A}$.
\end{itemize}


\paragraph{Kurzschreibweisen:}\label{Kurzschreibweisen1}
Für das Urbild eines Ereignisses bezüglich einer Zufallsvariable wird eine Kurzschreibweise definiert. So ist zum Beispiel $\simpleset{3\leq X\leq 4}\coloneqq X^{-1}([3,4])$. Auch eine Schreibweise wie $\simpleset{X=3}\cup\simpleset{X=4}\coloneqq X^{-1}(\simpleset{3,4})$ ist möglich.


\section{Diskrete Wahrscheinlichkeitsräume}
Einen einfachen Einstieg in die Wahrscheinlichkeitsräume stellen die endlichen Wahrscheinlichkeitsräume dar. Sie sind ein Sonderfall der diskreten Wahrscheinlichkeitsräume.
\subsection{Endliche Wahrscheinlichkeitsräume}
Ein Wahrscheinlichkeitsraum ist endlich, wenn $|\Omega|<\infty$ ist. Man verwendet dann üblicherweise $\mathcal A=\Pot(\Omega)$. Damit ist $\mathcal A$ unter allen Mengenoperationen abgeschlossen und für eine Zufallsvariable $X$ ist $X^{-1}(B)\in\mathcal A$ für alle $B\subseteq\R$.

\begin{definition}{Kolmogorov-Axiome}
	Einem endlichen Wahrscheinlichkeitsraum $(\Omega, P)$ liegen die folgenden Axiome zugrunde
	\begin{description}
		\item[\bf K1] $\forall A\subseteq \Omega: P(A)\geq 0$
		\item[\bf K2] $P(\Omega)=1$
		\item[\bf K3] $\forall A,B\subseteq \Omega, A\cap B=\emptyset: P(A\cup B)\coloneqq P(A+B)=P(A)+P(B) $
	\end{description}
\end{definition}

\begin{satz}{Folgerungen aus dem Axiomensystem}
	Einige Folgerungen aus den Kolmogorov-Axiomen
	\begin{enumerate}
		\item $P(\emptyset)=0$
		\item $P\left(\sum_i A_i\right)=\sum_i P(A_i)$
		\item $0\leq P(A)\leq 1$
		\item $\forall A\subseteq\Omega: P(\bar A)=1-P(A)$
		\item $A\subseteq B\subseteq C\Rightarrow P(A)\leq P(B)$
		\item $\forall A,B\subseteq \Omega: P(A\cup B)=P(A)+P(B)-P(A\cap B)$
		\item $P\left(\bigcup_i A_i\right)\leq \sum_i P(A_i)$
		\item $P(A_1\cup A_2\cup A_3)=P(A_1)+P(A_2)+P(A_3)-P(A_1\cap A_2)-P(A_1\cap A_3)-P(A_2\cap A_3)+P(A_1\cap A_2\cap A_3)$
	\end{enumerate}
\end{satz}

\paragraph{Beispiel:}
Wir werfen einen Oktaeder, das heißt $\Omega=\simpleset{1,\ldots,8}$ und $\mathcal A=\Pot \Omega$. Es gilt $p(\omega)=P(\simpleset{\omega})=\frac 18$ für alle $\omega \in\Omega$. Wir definieren das Ereignis $A_1=\simpleset{2,3,5,7}$, also alle Primzahlen auf dem Würfel.
\begin{equation*}
	P(A_1)=p(2)+p(3)+p(5)+p(7)=\sum_{\omega\in A_1}p(\omega)=\frac12.
\end{equation*}
Analog ist für $A_2=\simpleset{1,3,5,7}$ die Wahrscheinlichkeit $P(A_2)=\frac12$. Für die Vereinigung gilt
\begin{equation*}
	P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap A_2)=\frac12+\frac12-\frac38=\frac58.
\end{equation*}

\subsubsection{Zufallsvariablen auf endlichen Wahrscheinlichkeitsräumen}
Zufallsvariablen auf endlichen Wahrscheinlichkeitsräumen sind dann Abbildungen $X:\Omega \rightarrow \R$ mit der Verteilung $P^X:\Pot(X(\Omega))\rightarrow \R$ wobei $P^X(B)\coloneqq P(X^{-1}(B))$ für alle $B\subseteq X(\Omega)$ ist.
\paragraph{Kurzschreibweisen:}
\begin{itemize}
	\item Völlig analog zu den bereits eingeführten \hyperref[Kurzschreibweisen1]{Kurzschreibweisen} definieren wir
		\begin{align*}
		    P^X(B)=P(X^{-1}(B))&=P(\set{\omega\in\Omega}{X(\omega)\in B})\\
		    &=P(\simpleset{X\in B})\\
		    &=P(X\in B)
		\end{align*}
	\item Darauf aufbauend ist $P(a\leq X\leq b)$ für $a,b\in\R$ die Wahrscheinlichkeit $P^X([a,b]\cap X(\Omega))$.
	\item Und für einelementige Mengen ist $P(X=x)=P(X\in \simpleset{x})$.
\end{itemize}

\begin{definition}{Verteilungsfunktion}
	Eine Verteilungsfunktion der Zufallsvariablen $X$ ist eine Abbildung $F^X:\R\rightarrow\R$ mit $F^X(x)=P(X\leq x)$. Dies ist eine monoton steigende, rechtsseitig stetige Funktion.
\end{definition}


\begin{definition}{Laplace-Experiment}
	Ein Laplace-Experiment entspricht der intuitiven Gleichverteilung. Es handelt sich um einen Wahrscheinlichkeitsraum $(\Omega,P)$ wobei $p(\omega)=\frac 1{|\Omega|}$ für alle $\omega\in\Omega$ gilt.
	
	Das bedeutet $\forall A\subseteq \Omega:P(A)=\frac{|A|}{|\Omega|}$.
\end{definition}
\paragraph{Bemerkung:}
Für eine Zufallsvariable $X$ ist $P^X$ in der Regel keine Gleichverteilung, da $|X^{-1}(\simpleset{\omega})|>1$ vorkommen kann.

\paragraph{Beispiel:}
Für zwei Würfel ist der Grundraum $\Omega=\simpleset{1,\ldots,6}^2$ und die Wahrscheinlichkeit der Elementarereignisse ist $p((\omega_1,\omega_2))=\frac{1}{36}$ für alle $(\omega_1,\omega_2)\in\Omega$. Die Zufallsvariable $M(\omega_1,\omega_2)=\max\simpleset{\omega_1,\omega_2}$ beschreibt das Maximum der beiden Würfelergebnisse. Damit ist 
\begin{equation*}
	\begin{array}{r|cccccc}
		m&1&2&3&4&5&6\\
		\hline P(M=m)&\sfrac{1}{36}&\sfrac{3}{36}&\sfrac{5}{36}&\sfrac{7}{36}&\sfrac{9}{36}&\sfrac{11}{36}
	\end{array}
\end{equation*}

\begin{definition}{Erwartungswert einer Zufallsvariablen}
	Der Erwartungswert einer Zufallsvariablen $X$ auf dem Wahrscheinlichkeitsraum $(\Omega,P)$ ist
	\begin{align*}
		E(X)&=\sum_{\omega\in\Omega} X(\omega)*P(\simpleset{\omega})\\
		&=\sum_{\mathclap{x\in X(\Omega)}}x*P(X=x)
	\end{align*}
\end{definition}

\begin{satz}{Sätze zum Erwartungswert}
	Für zwei Zufallsvariablen $X$ und $Y$ auf dem Wahrscheinlichkeitsraum $(\Omega,P)$ und $a\in\R$ gilt
	\begin{description}
		\item[Linearität 1] $E(X+Y)=E(X)+E(Y)$
		\item[Linearität 2] $E(a*X)=a*E(X)$
		\item[Indikatorfunktion] $E(1_A)=P(A)$ mit $1_A=1\Leftrightarrow X(\omega)\in A$
		\item[Ordnung] $\forall \omega\in\Omega: X(\omega)\leq Y(\omega) \Rightarrow E(X)\leq E(Y)$
	\end{description}
\end{satz}

\begin{definition}{Varianz und Standardabw. von Zufallsvariablen}
	Die Varianz einer Zufallsvariablen $X$ auf dem Wahrscheinlichkeitsraum $(\Omega,P)$ ist
	\begin{equation*}
		V(X)=E((X-E(X))^2).
	\end{equation*}
	Entsprechend zur Standardabweichung von Merkmalen aus der Statistik ist die Standardabweichung einer Zufallsvariablen die Wurzel der Varianz
	\begin{equation*}
		\sigma(X)=\sqrt{V(X)}.
	\end{equation*}
\end{definition}

\begin{satz}{Sätze zur Varianz}
	Für eine Zufallsvariable $X$ und alle $\alpha,\beta\in\R$ gilt für die Varianz
	\begin{enumerate}
		\item $V(\alpha X + \beta) = \alpha^2 V(X)$ (Lineare Transformation)
		\item $V(X)=E(X^2)-(E(X))^2$ (Verschiebungssatz)
		\item $V(X)\geq 0$
		\item $V(X)=0\Leftrightarrow   \exists!a\in\R: P(X=a)=1$
	\end{enumerate}
\end{satz}

\paragraph{Beweis:}
\begin{enumerate}
	\item Varianz einer linear transformierten Zufallsvariable
	\begin{align*}
		V(\alpha X + \beta)&=E[((\alpha X+\beta)-E(\alpha X+\beta))^2]\\
		&=E[((\alpha X)-\alpha E(X))^2] &\text{(Linearität)}\\
		&=E[\alpha^2(X-E(X))^2]\\ 
		&=\alpha^2E[X-E(X)^2]\\
		V(\alpha X + \beta)&=\alpha^2V(X)
	\end{align*}
	\item Beweis des sog. Verschiebungssatzes mit der Linearität des Erwartungswerts
	\begin{align*}
		V(X)&=E[(X-E(X))^2]\\
		&=E[X^2-2X*E(X)+E(X)^2]\\
		&=E(X^2)-2E(X)*E(E(X))+E(X)^2\\
		&=E(X^2)-E(X)^2\\
	\end{align*}
\end{enumerate}


\begin{satz}{Tschebyscheff-Ungleichung}
	Für eine Zufallsvariable $X$ auf dem Wahrscheinlichkeitsraum $(\Omega,P)$ gilt für alle $\epsilon>0$
	\begin{equation*}
		P(|X-E(X)|\geq \epsilon)\leq \frac{V(X)}{\epsilon^2}
	\end{equation*}
\end{satz}

\begin{definition}{Bedingte Wahrscheinlichkeit}
	Auf Basis eines Zufallsexperiments wird die Wahrscheinlichkeit, dass ein Ereignis $B$ eintritt, während zusätzlich ein fest gewähltes Ereignis $A$ eintegtreten ist, \emph{bedingte Wahrscheinlichkeit} bezeichnet. Man interessiert sich also für die Wahrscheinlichkeit des Eregnisses $B$ unter der Bedingung, dass $A$ bereits eingetreten ist. In Zeichen ist die gesuchte Wahrscheinlichkeit
	\begin{equation*}
		P(B|A)=P_A(B)\coloneqq\frac{P(B\cap A)}{P(A)}
	\end{equation*}
\end{definition}

\subsubsection{Unabhängigkeit}
\begin{definition}{Unabhängigkeit von Ereignissen}
	Man nennt zwei Ereignisse $A,B\subset \Omega$ \emph{unabhängig}, wenn
	\begin{equation*}
		P(A\cap B)=P(A)*P(B)
	\end{equation*}
	gilt.
\end{definition}
\paragraph{Bemerkung:}
\begin{itemize}
	\item Die Unabhängigkeit von Ereignissen ist offensichtlich eine symmetrische Relation.
	\item Unabhängigkeit ist nicht mit stochastischer Kausalität zu verwechseln
	\item Sind zwei Ereignisse unabhängig, dann ist die bedingte Wahrscheinlichkeit gleich der totalen
	\begin{equation*}
		P(A|B)=P(A).
	\end{equation*}
	\item Sind zwei Ereignisse $A,B$ unvereinbar, das heißt $A\cap B=\emptyset$, sind sie genau dann unabhängig, wenn $P(A)=0$ oder $P(B)=0$ gilt.
	\item Sind zwei Ereignisse $A,B$ unabhängig, so sind auch deren Komplemente $\overline A, \overline B$ unabhängig.
\end{itemize}



{\color{red} UNABHÄNGIGKEIT VON ZV}


\begin{satz}{Erwartungswert bei unabhängigen ZV}
	Für zwei unabhängige Zufallsvariablen gilt für das Produkt der beiden
	\begin{equation*}
		E(X*Y)=E(X)*E(Y)
	\end{equation*}
\end{satz}
\paragraph{Beweis:}
Für das Produkt zweier unabhängiger Zufallsvariablen ist der Erwartungswert
\begin{align*}
	E(X*Y)&=\sum_{\mathclap{z\in(X*Y)(\Omega)}}z*P(X*Y=z)\\
	&=\sum_{\mathclap{(x,y)\in X(\Omega)\times Y(\Omega)}}xy*P(X=x \wedge Y=x)\\
	&=\sum_{x\in X(\Omega)}\Big[x*P(X=x) * \sum_{\mathclap{y\in Y(\Omega)}} y*P(Y=y)\Big] &\text{(Unabhängigkeit)}\\
	&=\sum_{x\in X(\Omega)}x*P(X=x)* \sum_{\mathclap{y\in Y(\Omega)}} y*P(Y=y)\\
	&=E(X)*E(Y)
\end{align*}


